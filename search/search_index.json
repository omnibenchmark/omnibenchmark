{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"omnibenchmark","text":"<p>omnibenchmark  provides community-driven, extensible and continuously-updating benchmarks. Omnibenchmark defines, runs and versions benchmark execution pipelines by leveraging a formal benchmark specification and a set of (reusable) benchmarking modules. Each benchmarking module implements a data processing step (typically a preprocessing step, method, or metric) and is stored as an independent git repository. Anyone can start (or contribute to) a benchmark. A contribution can be a relevant ground truth dataset, a missing method or metric, or update to existing modules. Omnibenchmark relies on widely used, free and open software solutions, including: git (version control system), Snakemake (workflow management system), easybuild (reproducible software environments), and apptainer (containerization), among others.</p>"},{"location":"#getting-started","title":"Getting started","text":"<ul> <li>Tutorials</li> <li>How to guides</li> </ul>"},{"location":"#preprints","title":"Preprints","text":"<ul> <li>Omnibenchmark (alpha) for continuous and open benchmarking in bioinformatics (2024)</li> <li>Building a continuous benchmarking ecosystem in bioinformatics (2024)</li> </ul>"},{"location":"howto/","title":"How-to guides","text":""},{"location":"howto/#add-parameters-to-a-benchmark-yaml","title":"Add parameters to a benchmark YAML","text":"<p>Module <code>P1</code> is parametrized and will run twice, once with <code>-a 0 -b 0.1</code> and second with <code>-a 1 -b 0.1</code>.</p> <pre><code>[snip]\nstages:\n    [snip]\n  - id: process\n    modules:\n      - id: P1\n        software_environment: \"R\"\n        parameters:\n          - values: [\"-a 0\", \"-b 0.1\"]\n          - values: [\"-a 1\", \"-b 0.1\"]\n        repository:\n          url: https://github.com/omnibenchmark-example/process.git\n          commit: ac5365e\n[snip]\n</code></pre>"},{"location":"howto/#exclude-certain-module-module-chains","title":"Exclude certain module-module chains","text":"<p>Module <code>M1</code> won't use inputs from module <code>D2</code>.</p> <pre><code>[snip]\nstages:\n- id: methods\n    modules:\n      - id: M1\n        software_environment: \"python\"\n        exclude: \n          - D2\n        repository:\n          url: https://github.com/omnibenchmark-example/method.git\n          commit: 1004cdd\n   [snip]\n</code></pre>"},{"location":"howto/#use-a-custom-singularity-container-to-run-methods","title":"Use a custom singularity container to run methods","text":"<p>We recommend building singularity containers using easybuild and <code>ob software singularity build --easyconfig [easyconfig]</code>. Still, it is possible to use any singularity container from an ORAS-compatible registry (could be a GitLab registry), or available locally as a SIF file.</p> <pre><code>---\nid: bench1\n\n[snip]\n\nsoftware_environments:                                 \n  remote_custom_container:\n    description: \"A singularity container from a registry\"\n    ## update the path to an ORAS-compatible registry\n    apptainer: oras://registry.renkulab.io/izaskun.mallona/sing\n  local_custom_container:\n    description: \"A singularity container - locally available as a SIF\"\n    ## local path to a local SIF file\n    apptainer: /home/user/singularity_image.sif\n</code></pre>"},{"location":"howto/#choosing-the-right-software-backend-lmod-singularity-conda","title":"Choosing the right software backend (lmod, singularity, conda)","text":"<p>Omnibenchmark is written in python and depends only on pypi packages. Some of its dependencies, namely those related to (reproducible) software stack management, are OS-specific.</p> OS <code>envmodules</code> <code>singularity</code> <code>conda</code> <code>cernvmfs</code> <code>Linux</code> () <code>MacOS</code> () <code>Windows</code> <p>On Linux, software can be managed:</p> <ul> <li>Using the host's binaries. If relevant interpreters/software are in your $PATH (perhaps using a virtual environment, or directly), they're accessible to omnibenchmark.</li> <li>Using conda. For that mamba is required. We provide an <code>environment.yaml</code> to help building the environment. </li> <li>Using singularity. For that, apptainer is needed.</li> <li>Using environment modules (lmod). For that, lmod is needed.</li> </ul> <p>Similarly, on MacOS:</p> <ul> <li>Using the host's binaries. If relevant interpreters/software are in your $PATH (perhaps using a virtual environment, or directly), they're accessible to omnibenchmark.</li> <li>Using conda. For that mamba is required. We provide an <code>environment.yaml</code> to help building the environment. </li> <li>Using singularity. It won't work unless using a virtual machine to provide a Linux-friendly host to singularity.</li> <li>Using environment modules (lmod). For that, lmod needs to be installed.</li> </ul> <p>We haven't fully tested omnibenchmark on Windows, but we would recommend using the  Windows Subsystem for Linux (WSL).</p>"},{"location":"howto/#enabling-networking-in-singularity-containers","title":"Enabling networking in singularity containers","text":"<p>(Updating)</p>"},{"location":"news/","title":"News","text":""},{"location":"news/#march-2025","title":"March 2025","text":"<ul> <li>Omnibenchmark was part of the Modern Benchmarking conference in Ascona, Swizerland:</li> <li>Omnibenchmark for continuous and open benchmarking (poster)</li> <li>How to build a continuous benchmarking ecosystem in bioinformatics? (poster)</li> <li>From solo to community benchmarking (slides)</li> </ul>"},{"location":"news/#january-2025","title":"January 2025","text":"<ul> <li>Release candidate v0.1.0-rc.3</li> </ul>"},{"location":"news/#december-2024","title":"December 2024","text":"<ul> <li>Release candidate v0.1.0-rc.2</li> </ul>"},{"location":"news/#september-2024","title":"September 2024","text":"<ul> <li>Release candidate v0.1.0-rc.1</li> <li>Preprint Omnibenchmark (alpha) for continuous and open benchmarking in bioinformatics</li> <li>Preprint Building a continuous benchmarking ecosystem in bioinformatics</li> </ul>"},{"location":"philosophy/","title":"Philosophy","text":"<p>Omnibenchmark is an automated benchmarking system.</p> <p>What actually is a benchmark, precisely? Here, a benchmark is thought of as a conceptual entity that roughly encompasses all the components of a study to understand the performance of a set of (computational) methods for a given task (here, the primary context is computational methods, but the concept can be applied to a broader scope, e.g., evaluation of laboratory-based methods). </p> <p>A benchmark requires a well-defined task, such as inferring what set of genes are differentially expressed from a certain type of data (e.g., gene expression from RNA sequencing counts) or whether a high-dimensional dataset can be clustered into the 'correct' cell types (the definition of this correctness, or ground-truth, should be precisely defined in advance). Given a well-defined task, a benchmark consists of benchmark components (datasets, preprocessing steps, methods, metrics) and ideally a benchmark system is used to organize and orchestrate the running of the benchmark, and create the benchmark artifacts (code snapshots, file outputs that will be shared, results tables). Ultimately, the downstream results and interpretations, which may involve rankings and various additional analyses, will be derived from these artifacts. Such a system should be public, open and allow contributions. Importantly, at the outset of a benchmark, a benchmark definition could be envisioned, which could give a formal specification of the entire set of components (and the pattern of artifacts to be generated). The benchmark definition can be expressed as a single YAML file that specifies the scope and topology of components to include, details of the repositories (code implementations with versions), the instructions to create software environments (accessible across compute architectures), and the parameters used.</p> <p>Similarly, a benchmark definition in terms of layers and their corresponding challenges and opportunities within each layer can be imagined.</p> <p>There are various ways to structure the running of a benchmark study: a benchmark can be run by a single person on a laptop (or a server), it can be organized as a challenge or within a hackathon (typically a group of people that come together to competitively assess their approaches and/or populate benchmark components).</p>"},{"location":"philosophy/#community","title":"Community","text":"<p>Different roles and ways that people can contribute to or interact with a benchmark system have been defined. </p> <p>The benchmarker is responsible for planning and coordinating a benchmark, defining the task, possibly splitting it into subtasks or processing stages, and defining the data formats across the stages; the benchmarker also brings an authority role of how to review and approve contributions. </p> <p>The contributors curate and add content to the benchmark, which could be new datasets, analysis methods or evaluation metrics, adhering to the guidelines set up by the benchmarker. Finally, the viewers of benchmark results are users who retrieve one or more of the artifacts (datasets, intermediate results, or metric scores). This could span a range of use cases, including the data analyst choosing which method to use for a specific application, an instructor retrieving a curated dataset for teaching purposes, or a methods researcher prototyping their method.</p>"},{"location":"philosophy/#software","title":"Software","text":"<p>Software plays an important role in the current scientific landscape, since it is widely used throughout the scientific process, including data collection, simulation, analysis and reporting. Researchers are encouraged to publish their data and code to enhance transparency and reproducibility in their work. Ideally, this would increase the adoption rate by the wider scientific community. In practice, however, it is sometimes easier to develop new code than to reuse existing software. This leads to the phenomenon of academic abandonware, where projects are forgotten in code repositories, exacerbating the reproducibility crisis in research. One common reason for these abandoned projects is their failure to adhere to the FAIR (Findable, Accessible, Interoperable, Reusable) principles <sup>1</sup>. Another common cause is the life cycle inherent to academic practice: after delivering a research output, there are often no resources to maintain a software package in the longer term; left alone, the likelihood of a particular library compiling or being able to run after 5-10 years will drastically decrease.</p> <p>Operational costs are given by the expected compute and storage requirements. In the context of benchmarking, predicting CPU and memory usage is challenging. The methods under evaluation typically have variable workloads, and their performance can fluctuate significantly. Benchmark designers could impose resource limits, thereby evaluating the methods under different constraints. Storage costs, on the other hand, are more predictable and can be directly influenced by design decisions of the benchmark (e.g., low retention for re-generatable artifacts, cold storage (i.e. Amazon Glacier, Azure Archive Storage) for code archives and software images, public repositories for method performance artifacts). Cold storage, in particular, is a cost-effective solution for the long-term storage of infrequently accessed data. Developments in IT infrastructure have led to the rise of cloud service providers and the commoditization of standardized compute and storage solutions, enabling these cost optimizations. </p> <p>Storage costs for a benchmarking system can be efficiently managed by implementing specific storage and retention policies for different types of data. First, datasets and intermediary artifacts, which are often not the primary focus, can be managed with a low retention policy. Since these items are typically not crucial in the long term and can be easily recomputed, this approach helps reduce storage costs. Second, code archives and software environment images can be stored in cold storage, ensuring they remain archived and accessible when needed without incurring high costs. Although retaining method code and software environments may seem unnecessary, taking snapshots of benchmark dependencies helps mitigate issues related to deadlinks and ensures mid-term reproducibility. Finally, benchmark artifacts can be managed in several ways: they can be stored for the long term, or alternatively, signed with a cryptographic key for later verification, with the responsibility for long-term storage delegated to the end user.</p>"},{"location":"philosophy/#reproducible-software-environments","title":"Reproducible software environments","text":"<p>Software reproducibility comes from controlling the triad of data, code, and environment. Computationally, a benchmark executes code that transforms input data into outputs: this execution takes place and is affected by the execution environment; the environment encompasses the base system (OS, compiler toolchains, libraries) and a set of configurations. The benchmark definition should control as much as possible of the execution environment.</p> <p>Leaving data aside, it can be useful to divide the codebase used into three distinct categories:</p> <ul> <li>The benchmark system itself has an impact on operational costs, such as storage needs, execution platform, etc. For example, it imposes a certain tech stack and base dependencies, and orchestrates the execution of a benchmark plan; usually, the system mandates the choice of a particular workflow manager. At this level, any requirements for input/output formats and shapes are defined; different systems can impose different constraints.</li> <li>Contributions related to individual benchmark components (e.g., datasets, methods, metrics) are expected to be small, typically short scripts that process data and wrap functionality by importing external libraries (where the component implementation itself is developed). Even so, imposing good practices at this level (output validation, testing for abnormal termination, ability to run with a subset of input data) can be beneficial to increase the quality and maintainability of the contributions. The toolset can also enforce validation of metadata annotations (authorship, versioning) for each contributed module.</li> <li>The software dependencies have easily the biggest impact on replicability and maintainability: archiving all combinations of a large dependency tree, across an arbitrary number of base OS images, will exponentially increase retention and maintenance costs. Turning the long-term replicability of a benchmark into a tractable problem is linked to choosing a sane software management system. Several software management systems have emerged to address these problems, from containerization (i.e. apptainer) and automating reproducible and efficient software management (i.e. easybuild <sup>2</sup>, Spack <sup>3</sup>), to software distribution (CernVM-FS) <sup>4</sup> and comprehensive initiatives to both build and distribute software (computecanada <sup>5</sup>, EESSI <sup>6</sup>).</li> </ul> <ol> <li> <p>Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E Bourne, and others. The fair guiding principles for scientific data management and stewardship. Scientific data, 3(1):1\u20139, 2016.\u00a0\u21a9</p> </li> <li> <p>Kenneth Hoste, Jens Timmerman, Andy Georges, and Stijn De Weirdt. Easybuild: building software with ease. In 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, 572\u2013582. IEEE, 2012.\u00a0\u21a9</p> </li> <li> <p>Todd Gamblin, Matthew LeGendre, Michael R Collette, Gregory L Lee, Adam Moody, Bronis R De Supinski, and Scott Futral. The spack package manager: bringing order to hpc software chaos. In Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, 1\u201312. 2015.\u00a0\u21a9</p> </li> <li> <p>Jakob Blomer, Predrag Buncic, and Ren\u00e9 Meusel. The cernvm file system. CERN, Geneva, Switzerland, Tech. Rep, pages 2\u20131, 2013.\u00a0\u21a9</p> </li> <li> <p>Maxime Boissonneault, Bart E Oldeman, and Ryan P Taylor. Providing a unified software environment for canada's national advanced computing centers. In Proceedings of the Practice and Experience in Advanced Research Computing on Rise of the Machines (learning), pages 1\u20136. 2019.\u00a0\u21a9</p> </li> <li> <p>Bob Dr\u00f6ge, Victor Holanda Rusu, Kenneth Hoste, Caspar van Leeuwen, Alan O'Cais, and Thomas R\u00f6blitz. Eessi: a cross-platform ready-to-use optimised scientific software stack. Software: Practice and Experience, 53(1):176\u2013210, 2023.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/","title":"CLI reference","text":""},{"location":"reference/#ob","title":"ob","text":"<p>OmniBenchmark Command Line Interface (CLI).</p> <p>Usage:</p> <pre><code>ob [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  --version             Show the version and exit.\n  --help                Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>info: List benchmarks and/or information about them.</li> <li>run: Run benchmarks or benchmark modules.</li> <li>software: Manage and install benchmark-specific software.</li> <li>storage: Manage remote storage.</li> </ul>"},{"location":"reference/#info","title":"info","text":"<p>List benchmarks and/or information about them.</p> <p>Usage:</p> <pre><code>ob info [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  --help                Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>computational: Export computational graph to dot format.</li> <li>diff: Show differences between 2 benchmark versions.</li> <li>list-versions: List all available benchmarks versions at a specific endpoint.</li> <li>topology: Export benchmark topology to mermaid diagram format.</li> </ul>"},{"location":"reference/#computational","title":"computational","text":"<p>Export computational graph to dot format.</p> <p>Usage:</p> <pre><code>ob info computational [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#diff","title":"diff","text":"<p>Show differences between 2 benchmark versions.</p> <p>Usage:</p> <pre><code>ob info diff [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  -v1, --version1 TEXT  Reference version.  [required]\n  -v2, --version2 TEXT  Version to compare with.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#list-versions","title":"list-versions","text":"<p>List all available benchmarks versions at a specific endpoint.</p> <p>Usage:</p> <pre><code>ob info list-versions [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#topology","title":"topology","text":"<p>Export benchmark topology to mermaid diagram format.</p> <p>Usage:</p> <pre><code>ob info topology [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#run","title":"run","text":"<p>Run benchmarks or benchmark modules.</p> <p>Usage:</p> <pre><code>ob run [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  --help                Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>benchmark: Run a benchmark as specified in the yaml.</li> <li>module: </li> <li>validate: Validate a benchmark yaml.</li> </ul>"},{"location":"reference/#benchmark","title":"benchmark","text":"<p>Run a benchmark as specified in the yaml.</p> <p>Usage:</p> <pre><code>ob run benchmark [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug            Enable debug mode\n  -b, --benchmark PATH            Path to benchmark yaml file or benchmark id.\n                                  [required]\n  -c, --cores INTEGER             Use at most N CPU cores in parallel. Default\n                                  is 1.\n  -u, --update                    Force re-run execution for all modules and\n                                  stages.\n  -d, --dry                       Dry run.\n  -k, --continue-on-error         Go on with independent jobs if a job fails\n                                  (--keep-going in snakemake).\n  -l, --local                     Execute and store results locally. Default\n                                  False.\n  --keep-module-logs / --no-keep-module-logs\n                                  Keep module-specific log files after\n                                  execution.\n  --task-timeout TEXT             Timeout for each separate task execution\n                                  (local only). Do note that total runtime is\n                                  not additive.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/#module","title":"module","text":"<p>Run a specific module that is part of the benchmark.</p> <p>Usage:</p> <pre><code>ob run module [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH            Path to benchmark yaml file or benchmark id.\n                                  [required]\n  -m, --module TEXT               Module id to execute  [required]\n  -i, --input_dir PATH            Path to the folder with the appropriate\n                                  input files.\n  -d, --dry                       Dry run.\n  -u, --update                    Force re-run execution for all modules and\n                                  stages.\n  -k, --continue-on-error         Go on with independent jobs if a job fails\n                                  (--keep-going in snakemake).\n  --keep-module-logs / --no-keep-module-logs\n                                  Keep module-specific log files after\n                                  execution.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/#validate","title":"validate","text":"<p>Validate a benchmark yaml.</p> <p>Usage:</p> <pre><code>ob run validate [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#software","title":"software","text":"<p>Manage and install benchmark-specific software.</p> <p>Usage:</p> <pre><code>ob software [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  --help                Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>check: Check whether the component {what} is available.</li> <li>conda: Manage and install software using conda.</li> <li>module: Manage and install software using Easybuild</li> <li>singularity: Manage and install software using Singularity.</li> </ul>"},{"location":"reference/#check","title":"check","text":"<p>Check whether the component {what} is available.</p> <p>Usage:</p> <pre><code>ob software check [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -w, --what TEXT  Binary/functionality to check:\n\n                   --what singularity : singularity\n\n                   --what module      : module tool, typically lmod\n\n                   --what easybuild   : easybuild\n\n                   --what conda       : conda\n  --help           Show this message and exit.\n</code></pre>"},{"location":"reference/#conda","title":"conda","text":"<p>Manage and install software using conda.</p> <p>Usage:</p> <pre><code>ob software conda [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>pin: Pin all conda env-related dependencies versions using snakedeploy.</li> <li>prepare: Pin all conda envs needed for a given benchmark YAML.</li> </ul>"},{"location":"reference/#pin","title":"pin","text":"<p>Pin all conda env-related dependencies versions using snakedeploy.</p> <p>Usage:</p> <pre><code>ob software conda pin [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -e, --env PATH  Conda env YAML.  [required]\n  --help          Show this message and exit.\n</code></pre>"},{"location":"reference/#prepare","title":"prepare","text":"<p>Pin all conda envs needed for a given benchmark YAML.</p> <p>Usage:</p> <pre><code>ob software conda prepare [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Benchmark YAML.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#module_1","title":"module","text":"<p>Manage and install software using Easybuild</p> <p>Usage:</p> <pre><code>ob software module [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>build: Build a given easyconfig (and generates the relevant envmodules).</li> <li>prepare: Build all envmodules needed for a given benchmark YAML.</li> </ul>"},{"location":"reference/#build","title":"build","text":"<p>Build a given easyconfig (and generates the relevant envmodules).</p> <p>Usage:</p> <pre><code>ob software module build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -e, --easyconfig TEXT  Easyconfig.  [required]\n  -p, --threads INTEGER  Number of threads.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/#prepare_1","title":"prepare","text":"<p>Build all envmodules needed for a given benchmark YAML.</p> <p>Usage:</p> <pre><code>ob software module prepare [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH   Benchmark YAML.  [required]\n  -p, --threads INTEGER  Number of threads.\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/#singularity","title":"singularity","text":"<p>Manage and install software using Singularity.</p> <p>Usage:</p> <pre><code>ob software singularity [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --help  Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>build: Build a singularity (fakeroot) image for a given easyconfig.</li> <li>prepare: Build all singularity (fakeroot) images needed for a benchmark.</li> <li>push: Pushes a singularity SIF file to an ORAS-compatible registry.</li> </ul>"},{"location":"reference/#build_1","title":"build","text":"<p>Build a singularity (fakeroot) image for a given easyconfig.</p> <p>Usage:</p> <pre><code>ob software singularity build [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -e, --easyconfig TEXT  Easyconfig.  [required]\n  --help                 Show this message and exit.\n</code></pre>"},{"location":"reference/#prepare_2","title":"prepare","text":"<p>Build all singularity (fakeroot) images needed for a benchmark.</p> <p>Usage:</p> <pre><code>ob software singularity prepare [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Benchmark YAML.  [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#push","title":"push","text":"<p>Pushes a singularity SIF file to an ORAS-compatible registry.</p> <p>Usage:</p> <pre><code>ob software singularity push [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -u, --docker_username TEXT  Docker username.  [required]\n  -p, --docker_password TEXT  Docker password.  [required]\n  -s, --sif PATH              Path to the Singularity SIF file.  [required]\n  -o, --oras TEXT             Registry's ORAS static URL, for instance oras://\n                              registry.mygitlab.ch/myuser/myproject:mytag.\n                              [required]\n  --help                      Show this message and exit.\n</code></pre>"},{"location":"reference/#storage","title":"storage","text":"<p>Manage remote storage.</p> <p>Usage:</p> <pre><code>ob storage [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  --help                Show this message and exit.\n</code></pre> <p>Subcommands</p> <ul> <li>archive: Archive a benchmark</li> <li>checksum: Generate md5sums of all benchmark outputs</li> <li>create-policy: Create a new policy for a benchmark.</li> <li>create-version: Create a new benchmark version.</li> <li>download: Download all or specific files for a benchmark.</li> <li>list: List all or specific files for a benchmark.</li> </ul>"},{"location":"reference/#archive","title":"archive","text":"<p>Archive a benchmark</p> <p>Usage:</p> <pre><code>ob storage archive [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH            Path to benchmark yaml file or benchmark id.\n                                  [required]\n  -c, --code                      Archive benchmarking code (repos).\n  -s, --software                  Archive software environments.\n  -r, --results                   Archive results files.\n  --compression [none|deflated|bzip2|lzma]\n                                  Compression method.  [default: none]\n  --compresslevel INTEGER         Compression level.\n  -n, --dry-run                   Do not create the archive, just show what\n                                  would be done.\n  -l, --local                     Execute and store results locally. Default\n                                  False.\n  --help                          Show this message and exit.\n</code></pre>"},{"location":"reference/#checksum","title":"checksum","text":"<p>Generate md5sums of all benchmark outputs</p> <p>Usage:</p> <pre><code>ob storage checksum [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#create-policy","title":"create-policy","text":"<p>Create a new policy for a benchmark.</p> <p>Usage:</p> <pre><code>ob storage create-policy [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#create-version","title":"create-version","text":"<p>Create a new benchmark version.</p> <p>Usage:</p> <pre><code>ob storage create-version [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#download","title":"download","text":"<p>Download all or specific files for a benchmark.</p> <p>Usage:</p> <pre><code>ob storage download [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  -t, --type TEXT       File types. Options: all, code, inputs, outputs, logs,\n                        performance.\n  -s, --stage TEXT      Stage to download files from.\n  -m, --module TEXT     Module to download files from.\n  -i, --id TEXT         File id to download.\n  -o, --overwrite       Overwrite existing files.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"reference/#list","title":"list","text":"<p>List all or specific files for a benchmark.</p> <p>Usage:</p> <pre><code>ob storage list [OPTIONS]\n</code></pre> <p>Options:</p> <pre><code>  --debug / --no-debug  Enable debug mode\n  -b, --benchmark PATH  Path to benchmark yaml file or benchmark id.\n                        [required]\n  -t, --type TEXT       File types. Options: all, code, inputs, outputs, logs,\n                        performance.\n  -s, --stage TEXT      Stage to list files for.\n  -i, --id TEXT         File id/type to list.\n  --help                Show this message and exit.\n</code></pre>"},{"location":"team/","title":"Contact","text":"<p>Omnibenchmark is being developed by the Mark D. Robinson's lab.</p>"},{"location":"team/#about-us","title":"About us","text":"<p>We are located in the Department of Molecular Life Sciences within the Faculty of Science at the University of Zurich.</p> <p>Research in our laboratory is focused on creating, comparing and understanding statistical methods and data science tools for processing and interpreting various types of genomic data. </p>"},{"location":"team/#team","title":"Team","text":"<p>Almut L\u00fctge, Anthony Sonrel, Charlotte Soneson, Daniel Incicau, Reto Gerber, Ben Carrillo, Izaskun Mallona, Mark D. Robinson</p>"},{"location":"team/#contact","title":"Contact","text":"<ul> <li>E-mail: mark.robinson (at) mls.uzh.ch</li> <li>Twitter: (https://twitter.com/markrobinsonca) (https://twitter.com/omnibenchmark)</li> <li>GitHub: omnibenchmark</li> </ul> <p>Address:</p> <pre><code>Department of Molecular Life Sciences\nUniversity of Zurich\nWinterthurerstrasse 190\nCH-8057 Zurich, Switzerland\n\nOffice Y11-G-05\nPhone  +41 44 635 48 48\nFax    +41 44 635 68 68\n</code></pre>"},{"location":"tutorial/","title":"Tutorials","text":""},{"location":"tutorial/#install-omnibenchmark","title":"Install omnibenchmark","text":"<p>Omnibenchmark runs on different operating systems (OS) and architectures. The installation procedure impacts omnibenchmark capabilities to manage software during benchmarking. We recommend installing using micromamba.</p> capabilities <code>system</code> <code>singularity</code> <code>lmod</code> <code>conda</code> <code>poetry</code> <code>pip</code> <code>mamba (e.g. micromamba)</code>"},{"location":"tutorial/#full-install-micromamba","title":"Full install (micromamba)","text":""},{"location":"tutorial/#apt-based-linux-on-amd64-architecture","title":"apt-based Linux on amd64 architecture","text":"<p>First, install micromamba, a faster conda manager and package solver.</p> ShellOutput <pre><code>\"${SHELL}\" &lt;(curl -L micro.mamba.pm/install.sh)\n</code></pre> <pre><code>% Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n                                 Dload  Upload   Total   Spent    Left  Speed\n  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n100  3059  100  3059    0     0   7059      0 --:--:-- --:--:-- --:--:--  7059\nMicromamba binary folder? [~/.local/bin] \nInit shell (bash)? [Y/n] Y\nConfigure conda-forge? [Y/n] n\nPrefix location? [~/micromamba] \nModifying RC file \"/home/user/.bashrc\"\nGenerating config for root prefix \"/home/user/micromamba\"\nSetting mamba executable to: \"/home/user/.local/bin/micromamba\"\nAdding (or replacing) the following in your \"/home/user/.bashrc\" file\n\n# &gt;&gt;&gt; mamba initialize &gt;&gt;&gt;\n# !! Contents within this block are managed by 'mamba init' !!\nexport MAMBA_EXE='/home/user/.local/bin/micromamba';\nexport MAMBA_ROOT_PREFIX='/home/user/micromamba';\n__mamba_setup=\"$(\"$MAMBA_EXE\" shell hook --shell bash --root-prefix \"$MAMBA_ROOT_PREFIX\" 2&gt; /dev/null)\"\nif [ $? -eq 0 ]; then\n    eval \"$__mamba_setup\"\nelse\n    alias micromamba=\"$MAMBA_EXE\"  # Fallback on help from mamba activate\nfi\nunset __mamba_setup\n# &lt;&lt;&lt; mamba initialize &lt;&lt;&lt;\n\nPlease restart your shell to activate micromamba or run the following:\\n\nsource ~/.bashrc (or ~/.zshrc, ~/.xonshrc, ~/.config/fish/config.fish, ...)\n</code></pre> <p>Then, clone omnibenchmark and install it in a new micromamba environment.</p> ShellOutput <pre><code>git clone git@github.com:omnibenchmark/omnibenchmark.git\n\ncd omnibenchmark\n\nmicromamba activate\nmicromamba create -n omnibenchmark\nmicromamba activate omnibenchmark\nmicromamba install -f test-environment.yml\n</code></pre> <pre><code>micromamba create -n omnibenchmark\nmicromamba activate omnibenchmark\nmicromamba install -f test-environment.yaml\nEmpty environment created at prefix: /home/user/micromamba/envs/omnibenchmark\ninfo     libmamba ****************** Backtrace Start ******************\ndebug    libmamba Loading configuration\ntrace    libmamba Compute configurable 'create_base'\ntrace    libmamba Compute configurable 'no_env'\ntrace    libmamba Compute configurable 'no_rc'\ntrace    libmamba Compute configurable 'rc_files'\ntrace    libmamba Compute configurable 'root_prefix'\n\n[snip]\n\n  + conda-libmamba-solver          24.7.0  pyhd8ed1ab_0           conda-forge     Cached\n  + mamba                           1.5.8  py312h9460a1c_0        conda-forge     Cached\n\n  Summary:\n\n  Install: 93 packages\n\n  Total download: 0 B\n\n\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\n\n\nConfirm changes: [Y/n] y\n\n[snip]\n\nSuccessfully built omnibenchmark omni-schema\nInstalling collected packages: toposort, throttler, stopit, sortedcontainers, pytz, plac, fastjsonschema, easybuild-framework, easybuild-easyconfigs, easybuild-easyblocks, distlib, connection-pool, async, appdirs, wrapt, tzdata, typing-extensions, traitlets, tabulate, soupsieve, smmap, six, shellingham, rpds-py, reretry, pyyaml, pytrie, pyparsing, pyjwt, pygments, pycryptodome, pulp, psutil, numpy, nodeenv, multidict, mdurl, MarkupSafe, lxml, jmespath, isort, iniconfig, immutables, identify, humanfriendly, hbreader, frozenlist, filelock, execnet, easybuild, dpath, docutils, datrie, coverage, configargparse, click, cfgv, attrs, argparse-dataclass, annotated-types, aiohappyeyeballs, yte, yarl, virtualenv, snakemake-interface-common, smart-open, referencing, python-swiftclient, python-dateutil, pytest, pynacl, pydantic-core, markdown-it-py, jupyter-core, jsonasobj2, json-flattener, jinja2, isodate, gitdb, docker, deprecated, cryptography, conda-inject, beautifulsoup4, argon2-cffi-bindings, aiosignal, testcontainers, snakemake-interface-storage-plugins, snakemake-interface-report-plugins, snakemake-interface-executor-plugins, rich, rdflib, pytest-xdist, pytest-split, pytest-logging, pytest-cov, pydantic, pre-commit, pandas, jsonschema-specifications, gitpython, bs4, botocore, argon2-cffi, aiohttp, typer, s3transfer, pygithub, prefixcommons, minio, jsonschema, curies, snakedeploy, prefixmaps, nbformat, boto3, snakemake, linkml-runtime, omni-schema, omnibenchmark\nSuccessfully installed MarkupSafe-2.1.5 aiohappyeyeballs-2.3.7 aiohttp-3.10.4 aiosignal-1.3.1 annotated-types-0.7.0 appdirs-1.4.4 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 argparse-dataclass-2.0.0 async-0.6.2 attrs-24.2.0 beautifulsoup4-4.12.3 boto3-1.35.0 botocore-1.35.0 bs4-0.0.2 cfgv-3.4.0 click-8.1.7 conda-inject-1.3.2 configargparse-1.7 connection-pool-0.0.3 coverage-7.6.1 cryptography-43.0.0 curies-0.7.10 datrie-0.8.2 deprecated-1.2.14 distlib-0.3.8 docker-7.1.0 docutils-0.21.2 dpath-2.2.0 easybuild-4.9.2 easybuild-easyblocks-4.9.2 easybuild-easyconfigs-4.9.2 easybuild-framework-4.9.2 execnet-2.1.1 fastjsonschema-2.20.0 filelock-3.15.4 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 hbreader-0.9.1 humanfriendly-10.0 identify-2.6.0 immutables-0.20 iniconfig-2.0.0 isodate-0.6.1 isort-5.13.2 jinja2-3.1.4 jmespath-1.0.1 json-flattener-0.1.9 jsonasobj2-1.0.4 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 jupyter-core-5.7.2 linkml-runtime-1.8.1 lxml-5.3.0 markdown-it-py-3.0.0 mdurl-0.1.2 minio-7.2.8 multidict-6.0.5 nbformat-5.10.4 nodeenv-1.9.1 numpy-2.1.0 omni-schema-0.0.1 omnibenchmark-0.1.0 pandas-2.2.2 plac-1.4.3 pre-commit-3.8.0 prefixcommons-0.1.12 prefixmaps-0.2.5 psutil-6.0.0 pulp-2.8.0 pycryptodome-3.20.0 pydantic-2.8.2 pydantic-core-2.20.1 pygithub-2.3.0 pygments-2.18.0 pyjwt-2.9.0 pynacl-1.5.0 pyparsing-3.1.2 pytest-8.3.2 pytest-cov-4.1.0 pytest-logging-2015.11.4 pytest-split-0.9.0 pytest-xdist-3.6.1 python-dateutil-2.9.0.post0 python-swiftclient-4.6.0 pytrie-0.4.0 pytz-2024.1 pyyaml-6.0.2 rdflib-7.0.0 referencing-0.35.1 reretry-0.11.8 rich-13.7.1 rpds-py-0.20.0 s3transfer-0.10.2 shellingham-1.5.4 six-1.16.0 smart-open-7.0.4 smmap-5.0.1 snakedeploy-0.10.0 snakemake-8.18.1 snakemake-interface-common-1.17.3 snakemake-interface-executor-plugins-9.2.0 snakemake-interface-report-plugins-1.0.0 snakemake-interface-storage-plugins-3.3.0 sortedcontainers-2.4.0 soupsieve-2.6 stopit-1.1.2 tabulate-0.9.0 testcontainers-4.8.0 throttler-1.2.2 toposort-1.10 traitlets-5.14.3 typer-0.12.4 typing-extensions-4.12.2 tzdata-2024.1 virtualenv-20.26.3 wrapt-1.16.0 yarl-1.9.4 yte-1.5.4\n</code></pre>"},{"location":"tutorial/#install-singularity-debootstrap-and-fakeroot","title":"Install singularity, debootstrap and fakeroot","text":"<p>Before proceeding, make sure to install apptainer (formerly singularity) as the containerization solution, as well as some other system-wide dependencies.</p> <p>After checking that apptainer is available, you should ensure debootstrap is available for building debian-based containers, and make sure to configure fakeroot with <code>singularity config fakeroot</code> to allow non-root users to simulate root privileges while managing containers.</p> <p>Do note that you will need <code>debootstrap</code> even if you're using a non-debian based linux.</p> <p>The following should get you covered:</p> <p>Finally, install apptainer (singularity) and further system dependencies. If apptainer is already installed, make sure debootstrap is also installed and fakeroot configured with <code>singularity config fakeroot</code>.</p> <pre><code>sudo apt install lua5.2 liblua5.2-dev lua-filesystem lua-posix tcl tcl-dev wget debootstrap software-properties-common\nsudo add-apt-repository -y ppa:apptainer/ppa\nsudo apt update\nsudo apt install openmpi-bin libopenmpi-dev apptainer\n</code></pre> <p>Check everything works with:</p> ShellOutput <pre><code>ob software check --what singularity\nob software check --what conda\nob software check --what easybuild\nob software check --what module\n</code></pre> <pre><code>Checking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['singularity', '--version'], returncode=0, stdout='singularity version 3.5.2\\n', std  err='')\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['conda', '--version'], returncode=0, stdout='conda 24.7.1\\n', stderr='')\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['eb', '--version'], returncode=0, stdout='This is EasyBuild 4.9.2 (framework: 4.9.2, easyblocks: 4.9.2) on host imlssherborne.\\n', stderr='')\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['type', 'module'], returncode=0, stdout='', stderr='')\n</code></pre>"},{"location":"tutorial/#macos","title":"MacOS","text":"<p>We assume your user has sudo power.</p> <p>First, install homebrew.</p> ShellOutput <pre><code>bash\n/bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\nexport PATH=/opt/homebrew/bin:$PATH\nbrew --version\n</code></pre> <pre><code>[snip]\nHomebrew 4.3.17\n</code></pre> <p>Then, install omnibenchmark dependencies, including lmod and micromamba.</p> ShellOutput <pre><code>brew upgrade\nbrew install coreutils\nbrew install gcc\nbrew install python\nbrew install git\nbrew install git-lfs\nbrew install lmod             \nif [ -f /usr/local/opt/lmod/init/profile ]; then\n    source /usr/local/opt/lmod/init/profile\nfi\nif [ -f /opt/homebrew/opt/lmod/init/profile ]; then\n    source /opt/homebrew/opt/lmod/init/profile\nfi\n\nbrew install wget\nbrew reinstall cmake\nbrew install micromamba\nmodule --version\nmicromamba --version\n</code></pre> <pre><code>Modules based on Lua: Version ---\n   by Robert McLay mclay@tacc.utexas.edu\n\n1.5.8\n</code></pre> <p>Clone omnibenchmark.</p> ShellOutput <pre><code>git clone https://github.com/omnibenchmark/omnibenchmark/\n\ncd omnibenchmark\n</code></pre> <pre><code>(no output)\n</code></pre> <p>Using micromamba, install omnibenchmark.</p> ShellOutput <pre><code>eval \"$(micromamba shell hook --shell bash)\"\nmicromamba create -n omnibenchmark\nmicromamba activate omnibenchmark\nmicromamba install -f mac-test-environment.yml\n</code></pre> <pre><code>[snip]\n    Successfully built omnibenchmark omni-schema\nInstalling collected packages: toposort, throttler, stopit, sortedcontainers, pytz, plac, fastjsonschema, easybuild-framework, easybuild-easyconfigs, easybuild-easyblocks, distlib, connection-pool, async, appdirs, wrapt, tzdata, typing-extensions, traitlets, tabulate, soupsieve, smmap, six, shellingham, rpds-py, reretry, pyyaml, pytrie, pyparsing, pyjwt, pygments, pycryptodome, pulp, psutil, numpy, nodeenv, multidict, mdurl, MarkupSafe, lxml, jmespath, isort, iniconfig, immutables, identify, humanfriendly, hbreader, frozenlist, filelock, execnet, easybuild, dpath, docutils, datrie, coverage, configargparse, click, cfgv, attrs, argparse-dataclass, annotated-types, aiohappyeyeballs, yte, yarl, virtualenv, snakemake-interface-common, smart-open, referencing, python-swiftclient, python-dateutil, pytest, pynacl, pydantic-core, markdown-it-py, jupyter-core, jsonasobj2, json-flattener, jinja2, isodate, gitdb, docker, deprecated, cryptography, conda-inject, beautifulsoup4, argon2-cffi-bindings, aiosignal, testcontainers, snakemake-interface-storage-plugins, snakemake-interface-report-plugins, snakemake-interface-executor-plugins, rich, rdflib, pytest-xdist, pytest-split, pytest-logging, pytest-cov, pydantic, pre-commit, pandas, jsonschema-specifications, gitpython, bs4, botocore, argon2-cffi, aiohttp, typer, s3transfer, pygithub, prefixcommons, minio, jsonschema, curies, snakedeploy, prefixmaps, nbformat, boto3, snakemake, linkml-runtime, omni-schema, omnibenchmark\nSuccessfully installed MarkupSafe-2.1.5 aiohappyeyeballs-2.3.7 aiohttp-3.10.4 aiosignal-1.3.1 annotated-types-0.7.0 appdirs-1.4.4 argon2-cffi-23.1.0 argon2-cffi-bindings-21.2.0 argparse-dataclass-2.0.0 async-0.6.2 attrs-24.2.0 beautifulsoup4-4.12.3 boto3-1.35.0 botocore-1.35.0 bs4-0.0.2 cfgv-3.4.0 click-8.1.7 conda-inject-1.3.2 configargparse-1.7 connection-pool-0.0.3 coverage-7.6.1 cryptography-43.0.0 curies-0.7.10 datrie-0.8.2 deprecated-1.2.14 distlib-0.3.8 docker-7.1.0 docutils-0.21.2 dpath-2.2.0 easybuild-4.9.2 easybuild-easyblocks-4.9.2 easybuild-easyconfigs-4.9.2 easybuild-framework-4.9.2 execnet-2.1.1 fastjsonschema-2.20.0 filelock-3.15.4 frozenlist-1.4.1 gitdb-4.0.11 gitpython-3.1.43 hbreader-0.9.1 humanfriendly-10.0 identify-2.6.0 immutables-0.20 iniconfig-2.0.0 isodate-0.6.1 isort-5.13.2 jinja2-3.1.4 jmespath-1.0.1 json-flattener-0.1.9 jsonasobj2-1.0.4 jsonschema-4.23.0 jsonschema-specifications-2023.12.1 jupyter-core-5.7.2 linkml-runtime-1.8.1 lxml-5.3.0 markdown-it-py-3.0.0 mdurl-0.1.2 minio-7.2.8 multidict-6.0.5 nbformat-5.10.4 nodeenv-1.9.1 numpy-2.1.0 omni-schema-0.0.1 omnibenchmark-0.1.0 pandas-2.2.2 plac-1.4.3 pre-commit-3.8.0 prefixcommons-0.1.12 prefixmaps-0.2.5 psutil-6.0.0 pulp-2.8.0 pycryptodome-3.20.0 pydantic-2.8.2 pydantic-core-2.20.1 pygithub-2.3.0 pygments-2.18.0 pyjwt-2.9.0 pynacl-1.5.0 pyparsing-3.1.2 pytest-8.3.2 pytest-cov-4.1.0 pytest-logging-2015.11.4 pytest-split-0.9.0 pytest-xdist-3.6.1 python-dateutil-2.9.0.post0 python-swiftclient-4.6.0 pytrie-0.4.0 pytz-2024.1 pyyaml-6.0.2 rdflib-7.0.0 referencing-0.35.1 reretry-0.11.8 rich-13.7.1 rpds-py-0.20.0 s3transfer-0.10.2 shellingham-1.5.4 six-1.16.0 smart-open-7.0.4 smmap-5.0.1 snakedeploy-0.10.0 snakemake-8.18.1 snakemake-interface-common-1.17.3 snakemake-interface-executor-plugins-9.2.0 snakemake-interface-report-plugins-1.0.0 snakemake-interface-storage-plugins-3.3.0 sortedcontainers-2.4.0 soupsieve-2.6 stopit-1.1.2 tabulate-0.9.0 testcontainers-4.8.0 throttler-1.2.2 toposort-1.10 traitlets-5.14.3 typer-0.12.4 typing-extensions-4.12.2 tzdata-2024.1 virtualenv-20.26.3 wrapt-1.16.0 yarl-1.9.4 yte-1.5.4\n</code></pre> <p>Check everything except singularity works with:</p> ShellOutput <pre><code>ob software check --what singularity ## should fail\nob software check --what conda\nob software check --what easybuild\nob software check --what module\n</code></pre> <pre><code>Checking software stack handlers / backends (singularity, easybuild, etc).\nFAILED\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['conda', '--version'], returncode=0, stdout='conda 24.7.1\\n', stderr='')\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['eb', '--version'], returncode=0, stdout='This is EasyBuild 4.9.2 (framework: 4.9.2, easyblocks: 4.9.2) on host imlssherborne.\\n', stderr='')\n\nChecking software stack handlers / backends (singularity, easybuild, etc).\nOK: CompletedProcess(args=['type', 'module'], returncode=0, stdout='', stderr='')\n</code></pre>"},{"location":"tutorial/#slim-install-python-package","title":"Slim install (python package)","text":"<p>You can install omnibenchmark as a python package. For that, you could use pip or poetry. To be able to run benchmarks you'll have to install <code>lmod</code> in your system. To be able to run benchmarks using singularity, you'll have to install <code>singularity</code> (apptainer) and <code>debootstrap</code> yourself.</p>"},{"location":"tutorial/#with-pip","title":"With pip","text":"<p>You might want to configure a virtualenv. Omnibenchmark requires python &gt;= 3.12.</p> Shell <pre><code>git clone https://github.com/omnibenchmark/omnibenchmark\ncd omnibenchmark\npip install .\n</code></pre>"},{"location":"tutorial/#with-poetry","title":"With poetry","text":"<p>Omnibenchmark requires python &gt;= 3.12.</p> Shell <pre><code>git clone https://github.com/omnibenchmark/omnibenchmark\ncd omnibenchmark\npoetry install\npoetry shell\n</code></pre>"},{"location":"tutorial/#install-software-using-envmodules","title":"Install software using envmodules","text":"<p>Omnibenchmark wraps easybuild to install easyconfigs.</p> <p>First search an appropriate easyconfig. We suggest installing <code>zlib-1.3</code> with the system toolchain - it should be quick. First, we make sure we can find an easyconfig named <code>zlib-1.3.1.eb</code>.</p> ShellOutput <pre><code>eb --search zlib-1.3\n</code></pre> <pre><code>== found valid index for /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs, so using it...\n * /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs/z/zlib/zlib-1.3.1-GCCcore-13.3.0.eb\n * /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs/z/zlib/zlib-1.3.1-GCCcore-14.1.0.eb\n * /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs/z/zlib/zlib-1.3.1.eb\n</code></pre> <p>Then, we install it with omnibenchmark.</p> ShellOutput <pre><code>ob software module build -e zlib-1.3.1.eb\n</code></pre> <pre><code>Installing software for zlib-1.3.1.eb using easybuild. It will take some time.\n== found valid index for /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs, so using it...\n== Temporary log file in case of crash /home/user/tmp/eb-t0ep4yar/eb-ny7hkqq2/easybuild-7my819c_.log\n== found valid index for /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs, so using it...\n== resolving dependencies ...\n== processing EasyBuild easyconfig /home/user/micromamba/envs/omnibenchmark/easybuild/easyconfigs/z/zlib/zlib-1.3.1.eb\n== building and installing zlib/1.3.1...\n== fetching files...\n== ... (took 2 secs)\n== creating build dir, resetting environment...\n== unpacking...\n== patching...\n== preparing...\n== configuring...\n== ... (took 1 secs)\n== building...\n== ... (took 6 secs)\n== testing...\n== installing...\n== ... (took 14 secs)\n== taking care of extensions...\n== restore after iterating...\n== postprocessing...\n== sanity checking...\n== cleaning up...\n== creating module...\n== permissions...\n== packaging...\n== COMPLETED: Installation ended successfully (took 26 secs)\n== Results of the build can be found in the log file(s) /home/user/.local/easybuild/software/zlib/1.3.1/easybuild/easybuild-zlib-1.3.1-20240820.082959.log\n\n== Build succeeded for 1 out of 1\n== Temporary log file(s) /home/user/tmp/eb-t0ep4yar/eb-ny7hkqq2/easybuild-7my819c_.log* have been removed.\n== Temporary directory /home/user/tmp/eb-t0ep4yar/eb-ny7hkqq2 has been removed.\nDONE\n</code></pre> <p>Then, we check whether we can find the associated module to this easyconfig.</p> ShellOutput <pre><code>source \"$LMOD_PKG\"/init/profile\nmodule use \"$HOME\"/.local/easybuild/modules/all\nmodule spider zlib\n</code></pre> <pre><code>module use \"$HOME\"/.local/easybuild/modules/all\nmodule spider zlib\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n  zlib: zlib/1.3.1\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library\n      for use on virtually any computer hardware and operating system. \n\n\n    This module can be loaded directly: module load zlib/1.3.1\n\n    Help:\n\n      Description\n      ===========\n      zlib is designed to be a free, general-purpose, legally unencumbered -- that\n       is, not covered by any patents -- lossless data-compression library for use\n       on virtually any computer hardware and operating system.\n\n\n      More information\n      ================\n       - Homepage: https://www.zlib.net/\n\n\n\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n  lib/zlib: lib/zlib/1.3.1\n----------------------------------------------------------------------------------------------------------------------------------------------------------\n    Description:\n      zlib is designed to be a free, general-purpose, legally unencumbered -- that is, not covered by any patents -- lossless data-compression library\n      for use on virtually any computer hardware and operating system. \n\n\n    This module can be loaded directly: module load lib/zlib/1.3.1\n\n    Help:\n\n      Description\n      ===========\n      zlib is designed to be a free, general-purpose, legally unencumbered -- that\n       is, not covered by any patents -- lossless data-compression library for use\n       on virtually any co\n</code></pre> <p>To load the module, we have to guess the module name from the easyconfig name. We are using a <code>flat</code> module naming known as EasyBuildMNS. So the module name is <code>zlib/1.3.1</code>.</p> <pre><code>module load zlib/1.3.1\nmodule list\n</code></pre> <p>We can unload the module with</p> <pre><code>module unload zlib/1.3.1\n</code></pre>"},{"location":"tutorial/#install-software-using-singularity","title":"Install software using singularity","text":"<p>To install software with easybuild inside a container, you can use <code>ob software singularity</code> commands.</p> <p>First search an appropriate easyconfig. We suggest installing <code>cowsay</code>, which is a nice say of saying hello world in the terminal, with the system toolchain. First, we make sure we can find an easyconfig named <code>cowsay</code>:</p> <pre><code>eb --search cowsay\n</code></pre> <p>As of this writing, there is only one version of the application, namely <code>cowsay-3.04.eb</code>.</p> <p>Then, we install it with omnibenchmark (do note that we'll be pinning the version too). This will generate a Singularity Image File (SIF), named <code>cowsay-3.04.eb.sif</code>:</p> <pre><code>ob software singularity build -e cowsay-3.04.eb\n</code></pre> <p>Now that we have built the SIF image, we can execute commands inside the singularity container. Let's do that as a way to verify that the image was correctly created.</p> ShellOutput <pre><code>singularity exec cowsay-3.04.eb.sif cowsay hello from singularity!\n</code></pre> <pre><code> _________________________\n&lt; hello from singularity! &gt;\n -------------------------\n        \\   ^__^\n         \\  (oo)\\_______\n            (__)\\       )\\/\\\n                ||----w |\n                ||     ||\n</code></pre>"},{"location":"tutorial/#design-a-benchmark-yaml","title":"Design a benchmark YAML","text":"<p>Benchmark specification files are written in YAML. They specify the formal dependencies of benchmark components, as well as some metadata (e.g. the repository containing their implementation, parameters to be run with, etc.).</p> <p>Let's construct a simple example benchmark, shaped as follows:</p> <ul> <li><code>D1</code> is a single (starting) dataset. (In real life, datasets will have meaningful   names, e.g. <code>semisimulation_smith_2019</code>).</li> <li><code>M1</code> and <code>M2</code> are methods. They process the dataset <code>D1</code> directly.   (Similarly, methods would also have proper names, e.g. <code>limma</code> or <code>linreg</code>).</li> <li><code>m1</code> and <code>m2</code> are metrics. They process the output of the methods <code>M1</code> and <code>M2</code>    directly. (Again, naming is flexible, we're keeping it short for clarity.)</li> </ul> <pre><code>flowchart LR\n  subgraph data\n    D1\n  end\n  subgraph methods\n    D1 --&gt; M1\n    D1 --&gt; M2\n  end\n  subgraph metrics\n    M1 --&gt; m1\n    M1 --&gt; m2\n    M2 --&gt; m1\n    M2 --&gt; m2\n  end</code></pre> <p>Benchmark specification files have a header and a body.</p>"},{"location":"tutorial/#benchmark-yaml-header","title":"Benchmark YAML header","text":"<p>Let's start with the header. </p> <pre><code>---\n## benchmark shortname\nid: bench1\n\n## benchmark description\ndescription: a simple benchmark\n\n## Benchmark version. `1.0`. This is our first attempt, so let's call it major version 1, minor version 0: `1.0`.\nversion: 1.0                                           \n\n## Benchmark builder/contact person\nbenchmarker: \"Mary the Benchmarker, mary@uzh.ch\"\n\n## Storage flavour for sharing results: currently only S3\nstorage_api: S3\n\n## S3 endpoint to share our benchmark results. \n##   `https://s3_object_storage_url.ch` does not exist, and we don't mind - \n##   not sharing results from our benchmark yet.\nstorage: https://s3_object_storage_url.ch              \n\n## Benchmark YAML schema/specification version. Currently `0.01`.\nbenchmark_yaml_spec: 0.01\n\n## License\n## license: MIT  # not yet part of the schema\n\n## The software backend used to run the benchmark\nsoftware_backend: apptainer\n\n## Software environment recipes associated to this benchmark. \n##  Suffice to say they are singularity images in some ORAS-compatible registry.\nsoftware_environments:                                 \n  R:\n    description: \"R 4.3.3 with gfbf-2023 toolchain\"\n    apptainer: http://registry.ch/R_4.3.3-gfbf-2023b.sif\n  python:\n    description: \"Python3.12.0 with gfbf-2023 toolchain\"\n    apptainer: http://registry.ch/python_vX-gfbf-2023b.sif\n</code></pre> <p>Hence, the header acts as a preamble, defining general attributes of the benchmark. The body contains the individual benchmark components (methods, metrics, etc) and their linking to each other.</p>"},{"location":"tutorial/#benchmark-yaml-body","title":"Benchmark YAML body","text":"<p>The benchmark body is structured in stages grouping benchmarking components that produce similarly shaped outputs and ingest similarly shaped inputs. That is: </p> <pre><code>flowchart LR\n    classDef thing fill:#f96\n    D1 -- produces --&gt; image\n    image:::thing -- \"is ingested by\" --&gt; M1\n    image:::thing -- \"is ingested by\" --&gt; M2\n    M1 -- produces --&gt; matrix1\n    M2 -- produces --&gt; matrix2\n    matrix1:::thing  -- \"is ingested by\" --&gt; m1\n    matrix1:::thing  -- \"is ingested by\" --&gt; m2\n    matrix2:::thing  -- \"is ingested by\" --&gt; m1\n    matrix2:::thing  -- \"is ingested by\" --&gt; m2  </code></pre> <p>In this example, <code>matrix1</code> and <code>matrix2</code> are similarly shaped, e.g. might be tab-separated files with some constraints, such as having a header and a rownames; and different from <code>image</code>, which might be a raster image in PNG format. We require <code>D1</code> to be part of a stage where modules produce images, and ingest no inputs; <code>M1</code> and <code>M2</code> to belong to a stage of image-ingesting, matrix-producing modules; and <code>m1</code> and <code>m2</code> to be part of a last stage of matrix-ingesting modules.</p> <p>Let's start with the first stage, containing <code>D1</code>. We will call it <code>data</code> (naming is flexible).</p> <pre><code>stages:\n    ## the stage name\n  - id: data\n    ##  here we have a single data stage with one single module, \n    ##  that outputs a single shape for the `data` stage.\n    modules:                                               \n        ## unique module id\n      - id: D1\n        ## module name in a longer form\n        name: \"Dataset 1\"\n        ## software environment to run this module; maps to the header `software_environments`\n        software_environment: \"python\"\n        ## the git-compatible remote, and a particular pinned commit\n        repository:\n          url: https://github.com/omnibenchmark-example/data.git\n          commit: 41aaa0a\n    ## output file paths for this stage members. In this simple case, the output from D1.\n    outputs:\n        ## output id\n      - id: data.image\n        ## output path. Wildcards will get dynamicly resoved to:\n        ##   input: the project root working directory\n        ##   stage: `data` (current stage id)\n        ##   module: `D1` (the only module in the `data` stage)\n        ##   params: `empty` (no parameters added)\n        ##   dataset: `D1` (module ids in initial stages - that is, the ones not ingesting inputs and only\n        ##     generating outputs, are reused as `dataset` wildcards)\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.png\"\n</code></pre> <p>Let's add another stage, for the modules <code>M1</code> and <code>M2</code>. This stage is not initial: its modules have both inputs and outputs.</p> <pre><code>    ## the stage name\n  - id: methods\n    ## a list of modules and their repositories, as above\n    modules:\n      - id: M1\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/method.git\n          commit: 1004cdd\n      - id: M2\n        ## notice this method runs in a container offering some R capabilities\n        software_environment: \"R\"\n        repository:\n          url: https://github.com/omnibenchmark-example/method2.git\n          commit: 10sg4cdd\n    ## input identifiers, refering to the `data stage` outputs\n    inputs:\n      - entries: data.image\n    ## stage-specific outputs\n    outputs:\n      - id: methods.matrix\n        ## output path. Wildcards will get dynamicly resoved to:\n        ##   input: not the project root anymore, but the path to the deepest file input\n        ##   stage: `methods` (current stage id)\n        ##   module: `M1` or `M2`\n        ##   params: `empty` (no parameters added)\n        ##   dataset: `D1` (here datasets refer to the initial stage above, not to the module name)\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.matrix.tsv.gz\"\n</code></pre> <p>You might be wondering: what does the wildcard <code>{input}</code> mean? The directory name (relative or full path) of <code>data.image</code>. This doesn't have to be modified by the user when writing the YAML; omnibenchmark will substitute paths appropriately. As a consequence, running module <code>D1</code> will generate files under the path template <code>{input}/{stage}/{module}/{params}/{dataset}.png</code>, that is:</p> <pre><code>./data/D1/default/D1.png\n</code></pre> <p>Hence, running modules <code>M1</code> and <code>M2</code> will produce files templated as <code>{input}/{stage}/{module}/{params}/{dataset}.matrix.tsv.gz</code>, which, given there is only one dataset <code>D1</code> available, will result in:</p> <pre><code>./data/D1/default/methods/M1/default/D1.matrix.tsv.gz\n./data/D1/default/methods/M2/default/D1.matrix.tsv.gz\n</code></pre> <p>Finally, we add the metrics stage containing modules <code>m1</code> and <code>m2</code>.</p> <pre><code>    ## the stage name\n  - id: metrics\n    ## a list of modules and their repositories, as above\n    modules:\n      - id: m1\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/metric.git\n          commit: 4504cdd\n      - id: m2\n        software_environment: \"R\"\n        repository:\n          url: https://github.com/omnibenchmark-example/metric2.git\n          commit: 7sg4cdd\n    ## input identifiers, refering to the `data stage` outputs\n    inputs:\n      - entries: methods.matrix\n    ## stage specific-outputs\n    outputs:\n      - id: metrics.json\n        ## output path. Wildcards will get dynamicly resoved to:\n        ##   input: not the project root anymore, but the path to the deepest file input (a method's output)\n        ##   stage: `metrics` (current stage id)\n        ##   module: `m1` or `m2`\n        ##   params: `empty` (no parameters added)\n        ##   dataset: `D1` (here datasets refer to the initial stage above, not to the module name)\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.json\"\n</code></pre> <p>Hence, running modules <code>m1</code> and <code>m2</code> will produce files templated as <code>{input}/{stage}/{module}/{params}/{dataset}.json</code>; given there is only one dataset <code>D1</code> and two methods <code>M1</code> and <code>M2</code> available, will result in the following outputs:</p> <pre><code>./data/D1/default/methods/M1/default/metrics/m1/default/D1.json\n./data/D1/default/methods/M2/default/metrics/m1/default/D1.json\n./data/D1/default/methods/M1/default/metrics/m2/default/D1.json\n./data/D1/default/methods/M2/default/metrics/m2/default/D1.json\n</code></pre> <p>The full benchmark YAML looks like this:</p> <pre><code>---\n## benchmark shortname\nid: bench1\n\n## benchmark description\ndescription: a simple benchmark\n\n## Benchmark version. `1.0`. This is our first attempt, so let's call it major version 1, minor version 0: `1.0`.\nversion: 1.0\n\n## Benchmark builder/contact person\nbenchmarker: \"Mary the Benchmarker, mary@uzh.ch\"\n\n## Storage flavour for sharing results: currently only S3\nstorage_api: S3\n\n## S3 endpoint to share our benchmark results. \n##   `https://s3_object_storage_url.ch` does not exist, and we don't mind - \n##   not sharing results from our benchmark yet.\nstorage: https://s3_object_storage_url.ch\n\n## Benchmark YAML schema/specification version. Currently `0.01`.\nbenchmark_yaml_spec: 0.01\n\n## License\n# license: MIT # not yet part of the schema\n\nsoftware_backend: apptainer\n\n## Software environment recipes associated to this benchmark. \n## Suffice to say they are singularity images in some ORAS-compatible registry.\nsoftware_environments:                                 \n  R:\n    description: \"R 4.3.3 with gfbf-2023 toolchain\"\n    apptainer: http://registry.ch/R_4.3.3-gfbf-2023b.sif\n  python:\n    description: \"Python3.12.0 with gfbf-2023 toolchain\"\n    apptainer: http://registry.ch/python_vX-gfbf-2023b.sif\n\nstages:\n  - id: data\n    modules:                                               \n      - id: D1\n        name: \"Dataset 1\"\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/data.git\n          commit: 41aaa0a\n    outputs:\n      - id: data.image\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.png\"\n\n  - id: methods\n    modules:\n      - id: M1\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/method.git\n          commit: 1004cdd\n      - id: M2\n        software_environment: \"R\"\n        repository:\n          url: https://github.com/omnibenchmark-example/method2.git\n          commit: 10sg4cdd\n    inputs:\n      - entries: data.image\n    outputs:\n      - id: methods.matrix\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.matrix.tsv.gz\"\n\n  - id: metrics\n    modules:\n      - id: m1\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/metric.git\n          commit: 4504cdd\n      - id: m2\n        software_environment: \"R\"\n        repository:\n          url: https://github.com/omnibenchmark-example/metric2.git\n          commit: 7sg4cdd\n    inputs:\n      - entries: methods.matrix\n    outputs:\n      - id: metrics.json\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.json\"\n</code></pre>"},{"location":"tutorial/#metric-collectors","title":"Metric collectors","text":"<p>The yaml stanzas above aim to scaffold a workflow by nesting inputs and outputs; that is, files contained within <code>{input}/{stage}/{module}/{params}</code> are produced by a given module <code>id</code> (and its associated <code>repository</code> and <code>software_environment</code>). These files can be further processed by other modules, i.e. <code>module_next</code>, so new files will be stored within <code>{input}/{stage}/{module}/{params}/{stage_next}/{module_next}/{params_next}</code>. Hence, lineages are linear, with an implicit provenance traceable by browsing the parent folder(s) of any folder and file. This can pose a challenge if multiple files (lineages) are meant to be gathered by a processing step.</p> <p>An independent syntax allows collecting multiple inputs across multiple folders and lineages to process them jointly. This usecase is typically needed when collecting metrics, that is, gathering all output files from some stage(s) to build a final aggregated report. Graphically, collection means adding the rightmost step  (<code>is collected by</code>) to the benchmarking workflow to produce <code>c1</code> (again, naming is flexible):</p> <pre><code>flowchart LR\n    classDef thing fill:#f96\n    D1 -- produces --&gt; image\n    image:::thing -- \"is ingested by\" --&gt; M1\n    image:::thing -- \"is ingested by\" --&gt; M2\n    M1 -- produces --&gt; matrix1\n    M2 -- produces --&gt; matrix2\n    matrix1:::thing  -- \"is ingested by\" --&gt; m1\n    matrix1:::thing  -- \"is ingested by\" --&gt; m2\n    matrix2:::thing  -- \"is ingested by\" --&gt; m1\n    matrix2:::thing  -- \"is ingested by\" --&gt; m2\n    m1 -- produces --&gt; M1_m1\n    m1 -- produces --&gt; M2_m1\n    m2 -- produces --&gt; M1_m2\n    m2 -- produces --&gt; M2_m2\n    M1_m1:::thing -- \"is collected by\\n(metric collector)\" --&gt; c1\n    M1_m2:::thing -- \"is collected by\\n(metric collector)\" --&gt; c1\n    M2_m1:::thing -- \"is collected by\\n(metric collector)\" --&gt; c1\n    M2_m2:::thing -- \"is collected by\\n(metric collector)\" --&gt; c1\n    c1 -- \"renders\" --&gt; report\n    report:::thing</code></pre> <p>The <code>is collected by</code> capability is specified within the benchmarking header (that is, before the <code>stages</code> stanzas) as a member of <code>metric_collectors</code> enumeration. (So multiple metric collectors can exist, if more than one stanza are added.) To specify a single metric collector reading all <code>metrics.json</code> outputs (while tracking their lineages, that is, their original dataset, methods, metric, parameters, etc): </p> <pre><code>metric_collectors:\n  - id: multiple_to_one\n    name: \"Single-backend (multiple) method outputs collector.\"\n    software_environment: \"python\"\n    repository:\n      url: https://github.com/imallona/clustering_report\n      commit: f1a5876\n    inputs:\n      - metrics.json\n    outputs:\n      - id: plotting.html\n        path: \"{input}/{name}/plotting_report.html\"\n</code></pre> <p>Similarly to any other module, the associated code to run the processing is stored as a git-compatible remote (with <code>url</code> and <code>commit id</code>). In the example above, <code>multiple_to_one</code> only generates one output, a report named <code>plotting.html</code> collecting all computed <code>metrics.json</code> files.</p>"},{"location":"tutorial/#validate-a-benchmark-yaml","title":"Validate a benchmark YAML","text":"<p>Let's save the benchmark above as a file named <code>benchmark_test.yaml</code>. Then we validate it with:</p> ShellOutput <pre><code>ob run validate -b benchmark_test.yaml\n</code></pre> <pre><code>Validating a benchmark yaml.\nBenchmark YAML file integrity check passed.\n</code></pre>"},{"location":"tutorial/#create-a-module-suitable-to-be-used-in-omnibenchmark","title":"Create a module suitable to be used in omnibenchmark","text":"<p>Any accesible git repository can host an omnibenchmark module. If it's convenient, you might want to push them to a remote in GitHub, Bitbucket, GitLab, etc. In reality, <code>omnibenchmark</code> just needs to be able to access the remote (clone or fetch), and be able to checkout your specified commit (so anything that works for your global git config should work for <code>omnibenchmark</code>).</p> <p>We provide an example set of modules for the benchmark example file at <code>tests/data/Benchmark_001.yaml</code>.</p> <p>As shown below, module D1 points to the GitHub repository example data at the commit <code>63b7b36</code>. (Incidentally, in theory you should also be able to specify any valid dynamic git reference, like <code>HEAD</code> or a <code>tag</code> or <code>branch</code> name).</p> <pre><code>stages:\n  - id: data\n    modules:\n      - id: D1\n        name: \"Dataset 1\"\n        software_environment: \"python\"\n        repository:\n          url: https://github.com/omnibenchmark-example/data.git\n          commit: 63b7b36\n    outputs:\n        ## output id\n      - id: data.image\n        ## output path. Wildcards will get dynamicly resoved to:\n        ##   input: the project root working directory\n        ##   stage: `data` (current stage id)\n        ##   module: `D1` (the only module `data stage` has)\n        ##   params: `empty` (no parameters added)\n        ##   dataset: `D1` (module ids in initial stages - that is, the ones not ingesting inputs and only\n        ##     generating outputs, are reused as `dataset` wildcards)\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.png\"\n</code></pre> <p>Hence, the git repository implementing module <code>D1</code> doesn't have any input, but generates one output. In this case, the repository implementing <code>D1</code> has a config file indicating the entrypoint is a python script named <code>entrypoint_data.py</code>:</p> <pre><code>[DEFAULT]\nSCRIPT=entrypoint_data.py\n</code></pre> <p><code>entrypoint_data.py</code> uses the python library <code>argparse</code> to receive two arguments when called from the command line:</p> <pre><code>parser.add_argument('--output_dir', type=str, help='output directory where dataset files will be saved.')) \nparser.add_argument('--name', type=str, help='name of the dataset')\n</code></pre> <p>That is, the output directory where the <code>data.image</code> output is generated, and the dataset (<code>D1</code>) name.</p> <p>Argument parsing aside, the <code>entrypoint_data.py</code> script structure is free: in this case, it materializes files with a dummy content.</p> <p>Let's inspect another module, this time running in R and also receiving inputs.</p> <pre><code>stages:\n  - id: some_intermediate_step\n    modules:\n      - id: process\n        exclude: [select_counts]\n        software_environment: \"R\"\n        repository:\n          url: https://github.com/omnibenchmark-example/process.git\n          commit: aeec1db\n    inputs:\n      - entries:\n          - data.meta\n          - data.counts\n    outputs:\n      - id: select_lognorm.selected\n        path: \"{input}/{stage}/{module}/{params}/{dataset}.txt.gz\"\n</code></pre> <p>So, in this case, the module <code>process</code> is likely to be implemented in R, receive three inputs, and produce one output. A dummy implementation is available at https://github.com/omnibenchmark-example/process.git. There, the config file indicates:</p> <pre><code>[DEFAULT]\nSCRIPT=entrypoint_process.R\n</code></pre> <p>so the script to be executed is named <code>entrypoint_process.R</code>. In this case, the script uses the R library <code>argparse</code> to provide a commandline interface:</p> <pre><code># Define argument parser\nparser &lt;- ArgumentParser(description=\"Process dataset files\")\n\n# Add arguments\nparser$add_argument(\"--output_dir\", \"-o\", dest=\"output_dir\", type=\"character\", help=\"output directory where files will be saved\")\nparser$add_argument(\"--name\", \"-n\", dest=\"name\", type=\"character\", help=\"name of the dataset\")\nparser$add_argument(\"--data.counts\", dest=\"data_counts\", type=\"character\", help=\"input file #1\")\nparser$add_argument(\"--data.meta\", dest=\"data_meta\", type=\"character\", help=\"input file #2\")\n</code></pre> <p>Notice these arguments match the YAML's: <code>data.counts</code> and <code>data.meta</code> are specified as inputs in the benchmark YAML; as before, <code>name</code> refers to the dataset name and <code>output_dir</code> to the path where outputs will be generated. As before, the script is free in structure - it implements some functionality, and can import other scripts as well, as long as it reads inputs and write outputs in a way compatible to the benchmark YAML specification.</p>"},{"location":"tutorial/#run-a-benchmark","title":"Run a benchmark","text":"<p>The benchmark <code>tests/data/Benchmark_001.yaml</code> above is a complex benchmark - but it runs quick enough. Let's try a dry run and inspect the rules that will be run:</p> ShellOutput <pre><code>ob run benchmark --benchmark tests/data/Benchmark_001.yaml  --cores 1 --local --dry\n</code></pre> <pre><code>[snip]\n\nINFO:snakemake.logging:\nJob stats:\njob                   count\n------------------  -------\nall                       1\ndata_D1_default           1\ndata_D2_default           1\nmethods_M1_default        5\nmethods_M2_param_0        5\nmethods_M2_param_1        5\nmetrics_m1_default       15\nmetrics_m2_default       15\nmetrics_m3_default       15\nprocess_P1_param_0        2\nprocess_P1_param_1        2\nprocess_P2_param_0        2\nprocess_P2_param_1        2\ntotal                    71\n\n[snip]\n</code></pre> <p>So it plans to run 71 jobs in total. Its methods are fast, so we can run it (it will take less than two minutes in most machines):</p> ShellOutput <pre><code>ob run benchmark --benchmark tests/data/Benchmark_001.yaml  --cores 1 --local\n</code></pre> <pre><code>[snip]\n\nresources: tmpdir=/home/imallona/tmp/eb-ge9tbg43\n\nINFO:snakemake.logging:\n[Thu Aug 29 10:23:23 2024]\nINFO:snakemake.logging:[Thu Aug 29 10:23:23 2024]\nFinished job 0.\nINFO:snakemake.logging:Finished job 0.\n71 of 71 steps (100%) done\nINFO:snakemake.logging:71 of 71 steps (100%) done\nComplete log: .snakemake/log/2024-08-29T102204.875104.snakemake.log\nWARNING:snakemake.logging:Complete log: .snakemake/log/2024-08-29T102204.875104.snakemake.log\nBenchmark run has finished successfully.\n</code></pre>"},{"location":"tutorial/#run-an-initial-module","title":"Run an initial module","text":"<p>The benchmark <code>tests/data/Benchmark_001.yaml</code> contains several initial steps which generate datasets and don't receive any input. To run these, we have to use the <code>ob run module</code> verb.</p> ShellOutput <pre><code>ob run module --benchmark tests/data/Benchmark_001.yaml --module D1\n</code></pre> <pre><code>Running module on a dataset provided in a custom directory.\nBenchmark YAML file integrity check passed.\nFound 1 workflow nodes for module D1.\nRunning module benchmark...\nAssuming unrestricted shared filesystem usage.\nWARNING:snakemake.logging:Assuming unrestricted shared filesystem usage.\nBuilding DAG of jobs...\nWARNING:snakemake.logging:Building DAG of jobs...\nUsing shell: /usr/bin/bash\nWARNING:snakemake.logging:Using shell: /usr/bin/bash\nProvided cores: 1 (use --cores to define parallelism)\nWARNING:snakemake.logging:Provided cores: 1 (use --cores to define parallelism)\nRules claiming more threads will be scaled down.\nWARNING:snakemake.logging:Rules claiming more threads will be scaled down.\nJob stats:\njob                count\n---------------  -------\nall                    1\ndata_D1_default        1\ntotal                  2\n\nreason: Missing output files: out/data/D1/default/D1.txt.gz, out/data/D1/default/D1_params.txt, out/data/D1/default/D1.meta.json; Code has changed since last execution\nresources: tmpdir=/home/imallona/tmp/eb-w7lf3kqk\nINFO:snakemake.logging:localrule data_D1_default:\n\n[snip]\n\nINFO:snakemake.logging:\n[Fri Sep  6 12:26:23 2024]\nINFO:snakemake.logging:[Fri Sep  6 12:26:23 2024]\nFinished job 0.\nINFO:snakemake.logging:Finished job 0.\n2 of 2 steps (100%) done\nINFO:snakemake.logging:2 of 2 steps (100%) done\nComplete log: .snakemake/log/2024-09-06T122622.173281.snakemake.log\nWARNING:snakemake.logging:Complete log: .snakemake/log/2024-09-06T122622.173281.snakemake.log\nModule run has finished successfully.\n</code></pre>"},{"location":"tutorial/#run-a-module-specifying-the-inputs","title":"Run a module specifying the inputs","text":"<p>The benchmark <code>tests/data/Benchmark_001.yaml</code> contains some data processing steps (e.g. <code>P1</code>) which take data inputs and produce outputs. To run only module <code>P1</code> only on data inputs already available locally at <code>out/data/D1/default/</code>, so results will be generated at <code>out/data/D1/default/process/P1/params/</code>, first double check the inputs are already where expected:</p> <pre><code>$ ls out/data/D1/default/\nD1.meta.json  D1_params.txt  D1.txt.gz\n</code></pre> <p>If not, run the whole benchmark first (with <code>ob run benchmark</code>). Once the input files are at <code>out/data/D1/default/</code>, run <code>ob run module</code> with:</p> ShellOutput <pre><code>ob run module --benchmark tests/data/Benchmark_001.yaml --module P1 --input out/data/D1/default\n</code></pre> <pre><code>Running module on a dataset provided in a custom directory.\nBenchmark YAML file integrity check passed.\nFound 2 workflow nodes for module P1.\nRunning module benchmark...\nAssuming unrestricted shared filesystem usage.\n\ninput: out/data/D1/default/D1.txt.gz, out/data/D1/default/D1.meta.json\n[snip]\n\nlocalrule all:\ninput: out/data/D1/default/D1.txt.gz, out/data/D1/default/D1.meta.json, out/data/D1/default/process/P1/param_0/D1.txt.gz\njobid: 0\nreason: Input files updated by another job: out/data/D1/default/process/P1/param_0/D1.txt.gz\nresources: tmpdir=/home/imallona/tmp/eb-unlssiuj\nINFO:snakemake.logging:localrule all:\ninput: out/data/D1/default/D1.txt.gz, out/data/D1/default/D1.meta.json, out/data/D1/default/process/P1/param_0/D1.txt.gz\njobid: 0\nreason: Input files updated by another job: out/data/D1/default/process/P1/param_0/D1.txt.gz\nresources: tmpdir=/home/imallona/tmp/eb-unlssiuj\n\nINFO:snakemake.logging:\n[Fri Sep  6 12:35:15 2024]\n\nINFO:snakemake.logging:[Fri Sep  6 12:35:15 2024]\nFinished job 0.\nINFO:snakemake.logging:Finished job 0.\n2 of 2 steps (100%) done\nINFO:snakemake.logging:2 of 2 steps (100%) done\nComplete log: .snakemake/log/2024-09-06T123513.568197.snakemake.log\nWARNING:snakemake.logging:Complete log: .snakemake/log/2024-09-06T123513.568197.snakemake.log\nModule run has finished successfully.\n</code></pre>"},{"location":"tutorial/#remote-storage-s3-aws-or-minio","title":"Remote storage - S3 (AWS or MinIO)","text":"<p>To restrict access to a dedicated bucket an access key with a specific policy have to generated.</p>"},{"location":"tutorial/#create-policy","title":"Create policy","text":"<p>Create new policy with <pre><code>ob storage create-policy --benchmark tests/data/Benchmark_001.yaml\n</code></pre> The output of this command needs to be added to either MinIO or AWS as described below.</p>"},{"location":"tutorial/#create-new-access-key","title":"Create new access key","text":""},{"location":"tutorial/#minio","title":"MinIO","text":"<p>In the MinIO Console navigate to 'Access Keys' and click 'Create access key'. Set 'Restrict beyond user policy' to 'ON'. Replace the displayed policy with the output of the above command. Optionally enter a name and a description. Click on <code>Create</code> and copy the access key and secret key.</p>"},{"location":"tutorial/#aws","title":"AWS","text":"<p>Create a new user. Create a new policy with the output of the above command. Attach policy to user. Create access key for user.</p>"},{"location":"tutorial/#save-access-key-information-locally-optional","title":"Save access key information locally (Optional)","text":"<p>Save the access key and secret key in a <code>&lt;CONFIG&gt;.json</code> file somewhere with the following format:</p> <pre><code>{\"access_key\": \"&lt;ACCESS_KEY&gt;\", \"secret_key\": \"&lt;SECRET_KEY&gt;\"}\n</code></pre>"},{"location":"tutorial/#usage","title":"Usage","text":"<p>To use the credentials to write to the remote storage the access key and secret key are passed to omnibenchmark with environment variables. If the credentials have been stored as described above the environment variable <code>OB_STORAGE_S3_CONFIG</code> can be set which is the name of the config file. For example:</p> <pre><code>OB_STORAGE_S3_CONFIG=&lt;CONFIG&gt;.json ob run benchmark -b tests/data/Benchmark_003.yaml\n</code></pre> <p>alternatively <code>OB_STORAGE_S3_ACCESS_KEY</code> and <code>OB_STORAGE_S3_SECRET_KEY</code> can be set. For example:</p> <pre><code>OB_STORAGE_S3_ACCESS_KEY=&lt;ACCESS_KEY&gt; OB_STORAGE_S3_SECRET_KEY=&lt;SECRET_KEY&gt; ob run benchmark -b tests/data/Benchmark_003.yaml\n</code></pre>"},{"location":"tutorial/#versioning","title":"Versioning","text":"<p>To version currently stored data in the remote (i.e. make it read-only) run the following: <pre><code>ob storage create-version -b tests/data/Benchmark_003.yaml\n</code></pre></p> <p>A second execution will result in an error since this version now already exists. To create a new version, first update the version in the Benchmark.yaml file and then rerun the above command.</p>"}]}