"""
Regression test for intermediate output re-generation behavior.

This test verifies that when an intermediate output is deleted,
running omnibenchmark again will correctly detect the missing file
and re-generate it.

Regression test for issue #277: deleting intermediate results does not trigger re-running of the benchmark

The test uses the data -> preprocessing -> methods pipeline (config 03)
which has clear intermediate outputs (preprocessing.processed files) that
sit between data.raw and methods.result stages.

Test scenario:
1. Run the full pipeline to completion
2. Delete one of the intermediate preprocessing outputs
3. Re-run the pipeline
4. Verify that the deleted intermediate output is re-generated

Expected behavior:
- The pipeline should detect the missing intermediate file
- It should re-run the preprocessing step to regenerate it
- It should continue with dependent steps (methods) if needed
- All outputs should be present after the second run
"""

import pytest
from pathlib import Path


@pytest.fixture
def intermediate_output_config():
    """Get the path to the config with intermediate outputs (preprocessing stage)."""
    config_path = (
        Path(__file__).parent / "configs" / "03_data_preprocessing_methods_metrics.yaml"
    )
    return config_path


@pytest.mark.e2e
def test_intermediate_output_regeneration_after_deletion(
    intermediate_output_config, tmp_path, bundled_repos, keep_files
):
    """
    Test that deleted intermediate outputs are correctly re-generated.

    This test verifies the behavior when an intermediate output file is deleted.
    The pipeline should:
    1. Detect the missing intermediate file
    2. Re-run the preprocessing step that generates it
    3. Re-run any dependent steps if needed
    4. Successfully complete with all outputs present

    The test uses the preprocessing pipeline which has this structure:
        data.raw -> preprocessing.processed -> methods.result -> metrics.summary

    Where preprocessing.processed files are intermediate outputs.
    """
    from tests.e2e.common import E2ETestRunner, filter_files_excluding_symlinked_dirs

    runner = E2ETestRunner(tmp_path, keep_files)

    # ========================================================================
    # STEP 1: Initial pipeline run - generate all outputs
    # ========================================================================
    config_file_in_tmp = runner.setup_test_environment(
        intermediate_output_config,
        "03_data_preprocessing_methods_metrics.yaml",
    )

    # Execute the full pipeline
    runner.execute_cli_command(
        config_file_in_tmp,
        ["--continue-on-error", "-y"],
        debug_label="initial run",
    )

    # Validate that the pipeline completed successfully
    runner.validate_results("03_data_preprocessing_methods_metrics")
    runner.verify_output_file_count(2, "*_data.json")  # 2 data files
    runner.verify_output_file_count(4, "*_preprocessed*.json")  # 4 preprocessing files
    runner.verify_output_file_count(8, "*_method*.json")  # 8 method result files

    if keep_files:
        print("\n=== After initial run ===")
        runner.debug_output_structure()

    # ========================================================================
    # STEP 2: Identify and delete one intermediate output
    # ========================================================================

    # Find all preprocessing outputs (intermediate files)
    preprocessing_outputs = filter_files_excluding_symlinked_dirs(
        runner.out_dir, "*_preprocessed*.json"
    )

    assert len(preprocessing_outputs) > 0, "No preprocessing outputs found to delete"

    # Select the first preprocessing output to delete
    # This file is an intermediate output: it's generated by the preprocessing stage
    # and consumed by the methods stage
    target_file_to_delete = preprocessing_outputs[0]
    relative_path = target_file_to_delete.relative_to(runner.out_dir)

    if keep_files:
        print("\n=== Deleting intermediate output ===")
        print(f"Target file: {relative_path}")
        print(f"Full path: {target_file_to_delete}")

    # Verify the file exists before deletion
    assert (
        target_file_to_delete.exists()
    ), f"Target file {relative_path} does not exist before deletion"

    # Store the modification time of a downstream file (method result)
    # to verify if it gets re-generated
    method_outputs = filter_files_excluding_symlinked_dirs(
        runner.out_dir, "*_method*.json"
    )
    downstream_file = None
    downstream_file_mtime_before = None

    # Find a method output that depends on the deleted preprocessing output
    # (they should share the same dataset/preprocessing lineage)
    for method_file in method_outputs:
        # Simple heuristic: if the preprocessing file path is contained in the method file path,
        # they're likely related (e.g., both under D1/P1/)
        if target_file_to_delete.parent.name in str(method_file):
            downstream_file = method_file
            downstream_file_mtime_before = downstream_file.stat().st_mtime
            if keep_files:
                print(
                    f"Found downstream file: {downstream_file.relative_to(runner.out_dir)}"
                )
            break

    # Delete the intermediate file
    target_file_to_delete.unlink()

    if keep_files:
        print(f"✓ Deleted: {relative_path}")
        print("=== End deletion ===\n")

    # Verify deletion succeeded
    assert not target_file_to_delete.exists(), f"Failed to delete {relative_path}"

    # ========================================================================
    # STEP 3: Re-run the pipeline
    # ========================================================================

    if keep_files:
        print("\n=== Re-running pipeline after deletion ===")

    runner.execute_cli_command(
        config_file_in_tmp,
        ["--continue-on-error", "-y"],
        debug_label="second run (after deletion)",
    )

    if keep_files:
        print("\n=== After second run ===")
        runner.debug_output_structure()

    # ========================================================================
    # STEP 4: Verify that the intermediate output was re-generated
    # ========================================================================

    # Check that the deleted file now exists again
    assert target_file_to_delete.exists(), (
        f"REGRESSION FAILURE: Intermediate output {relative_path} was not re-generated after deletion.\n"
        f"The pipeline should have detected the missing intermediate file and re-run the preprocessing step."
    )

    if keep_files:
        print(f"\n✓ SUCCESS: Intermediate output {relative_path} was re-generated")

    # Verify that all preprocessing outputs exist
    preprocessing_outputs_after = filter_files_excluding_symlinked_dirs(
        runner.out_dir, "*_preprocessed*.json"
    )
    assert (
        len(preprocessing_outputs_after) == 4
    ), f"Expected 4 preprocessing outputs after re-run, found {len(preprocessing_outputs_after)}"

    # Verify that downstream files were also re-generated if needed
    # (The modification time should be updated if the file was re-generated)
    if downstream_file is not None:
        downstream_file_mtime_after = downstream_file.stat().st_mtime
        # Note: We don't strictly require the downstream file to be regenerated,
        # as the pipeline might be smart enough to only regenerate the missing file.
        # But we document the behavior here for future reference.
        if keep_files:
            if downstream_file_mtime_after > downstream_file_mtime_before:
                print(
                    f"✓ Downstream file was re-generated: {downstream_file.relative_to(runner.out_dir)}"
                )
            else:
                print(
                    f"ℹ Downstream file was NOT re-generated (may be cached): {downstream_file.relative_to(runner.out_dir)}"
                )

    # ========================================================================
    # STEP 5: Validate complete pipeline results
    # ========================================================================

    # Validate that all expected outputs are correct
    runner.validate_results("03_data_preprocessing_methods_metrics")
    runner.verify_output_file_count(2, "*_data.json")
    runner.verify_output_file_count(4, "*_preprocessed*.json")
    runner.verify_output_file_count(8, "*_method*.json")

    if keep_files:
        print("\n✓ All validation checks passed")
        print("✓ Intermediate output re-generation test PASSED")
